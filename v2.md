# Chess Grand Prix Tracker v2 - Rewrite Plan

## Vision

Create a simple, reliable platform for tracking chess tournaments in Kenya, with a focus on the Grand Prix series. The system should provide accurate rankings, player statistics, and tournament information while being easy to maintain.

## Core Objectives

1. **Improve Data Reliability**: Enhance data storage and scraping reliability
2. **Enhance User Experience**: Create a clean, responsive interface
3. **Increase System Robustness**: Improve error handling
4. **Simplify Maintenance**: Make the system easier to update

## Technical Architecture

### Backend Evolution

#### Current (v1)
- Flask API
- SQLite database
- Direct scraping in request handlers
- Basic error handling

#### Proposed (v2)
- **Framework**: Migrate to FastAPI for improved performance and automatic OpenAPI docs
- **Database**: Maintain SQLite with improved schema and indexes
- **Deployment**: Fly.io with persistent volumes for database storage
- **API Design**: Simple RESTful API
- **Validation**: Pydantic models for data validation

### Frontend Evolution

#### Current (v1)
- Next.js with basic components
- Direct API calls in pages
- Limited error handling
- Basic responsive design

#### Proposed (v2)
- **Framework**: Maintain Next.js with App Router
- **Data Fetching**: Leverage React Server Components (RSC) for server-side data fetching
- **UI Components**: Continue with shadcn/ui component library
- **Error Handling**: Add error boundaries for client components
- **Responsive Design**: Improved mobile experience with simplified layouts

## Database Schema Redesign

### Tournaments Table
```sql
CREATE TABLE tournaments (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    short_name TEXT NOT NULL,
    location TEXT NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    rounds INTEGER NOT NULL,
    status TEXT NOT NULL, -- 'upcoming', 'ongoing', 'completed'
    chess_results_id TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add indexes for common queries
CREATE INDEX idx_tournaments_status ON tournaments(status);
CREATE INDEX idx_tournaments_date ON tournaments(start_date);
```

### Players Table
```sql
CREATE TABLE players (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    fide_id TEXT UNIQUE,
    name TEXT NOT NULL,
    federation TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add indexes for common queries
CREATE INDEX idx_players_federation ON players(federation);
CREATE INDEX idx_players_name ON players(name COLLATE NOCASE);
```

### PlayerRatings Table (New)
```sql
CREATE TABLE player_ratings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    player_id INTEGER NOT NULL,
    rating INTEGER NOT NULL,
    rating_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (player_id) REFERENCES players(id),
    UNIQUE(player_id, rating_date)
);

-- Add index for time-series queries
CREATE INDEX idx_player_ratings_date ON player_ratings(player_id, rating_date);
```

### Results Table
```sql
CREATE TABLE results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tournament_id TEXT NOT NULL,
    player_id INTEGER NOT NULL,
    rating INTEGER,
    points REAL NOT NULL,
    tpr INTEGER,
    has_walkover BOOLEAN DEFAULT 0,
    start_rank INTEGER,
    games_played INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tournament_id) REFERENCES tournaments(id),
    FOREIGN KEY (player_id) REFERENCES players(id),
    UNIQUE(tournament_id, player_id)
);

-- Add indexes for common queries
CREATE INDEX idx_results_tournament ON results(tournament_id);
CREATE INDEX idx_results_player ON results(player_id);
CREATE INDEX idx_results_tpr ON results(tpr);
```

### Games Table (New)
```sql
CREATE TABLE games (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tournament_id TEXT NOT NULL,
    round INTEGER NOT NULL,
    white_player_id INTEGER NOT NULL,
    black_player_id INTEGER NOT NULL,
    result TEXT NOT NULL, -- '1-0', '0-1', '½-½', '+/-', '-/+', etc.
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tournament_id) REFERENCES tournaments(id),
    FOREIGN KEY (white_player_id) REFERENCES players(id),
    FOREIGN KEY (black_player_id) REFERENCES players(id),
    UNIQUE(tournament_id, round, white_player_id, black_player_id)
);

-- Add indexes for common queries
CREATE INDEX idx_games_tournament_round ON games(tournament_id, round);
CREATE INDEX idx_games_white_player ON games(white_player_id);
CREATE INDEX idx_games_black_player ON games(black_player_id);
```

## Scraping Improvements

### Enhanced Scraper Architecture
1. **Modular Design**:
   - Separate scraper components for different page types
   - Reusable parsing functions for common elements

2. **Resilient Parsing**:
   - Multiple parsing strategies for each data element
   - Fallback mechanisms when primary parsing fails
   - Better error handling

3. **Validation**:
   - Data validation before storage
   - Consistency checks across different data sources

4. **Rate Limiting**:
   - Respectful crawling with delays between requests
   - Simple retry mechanism for failed requests

### Scraper Implementation Details

```python
# Example of improved scraper architecture

class BaseScraper:
    """Base class for all scrapers with common functionality"""
    def __init__(self, session=None):
        self.session = session or requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """Configure session with headers"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url):
        """Get page content with simple retry logic"""
        for attempt in range(3):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response.text
            except (requests.RequestException, requests.Timeout) as e:
                if attempt == 2:  # Last attempt
                    raise
                time.sleep(2 ** attempt)  # Simple exponential backoff
        
        return None  # Should never reach here due to the raise above

class TournamentScraper(BaseScraper):
    """Scraper for tournament details"""
    def get_tournament_info(self, tournament_id):
        """Get basic tournament information"""
        url = f"https://chess-results.com/tnr{tournament_id}.aspx?lan=1"
        html = self.get_page(url)
        
        # Try primary parsing strategy
        info = self._parse_tournament_info_primary(html)
        
        # If primary fails, try fallback
        if not info.get('name'):
            info = self._parse_tournament_info_fallback(html)
            
        return info
    
    def _parse_tournament_info_primary(self, html):
        """Primary parsing strategy for tournament info"""
        soup = BeautifulSoup(html, 'html.parser')
        # Implementation details...
        
    def _parse_tournament_info_fallback(self, html):
        """Fallback parsing strategy for tournament info"""
        # Alternative parsing approach...

class PlayerScraper(BaseScraper):
    """Scraper for player details and results"""
    # Implementation details...

class GameScraper(BaseScraper):
    """Scraper for individual game results"""
    # Implementation details...

# Simple coordinator function
def scrape_tournament(tournament_id, db_connection):
    """Scrape all data for a tournament"""
    tournament_scraper = TournamentScraper()
    player_scraper = PlayerScraper()
    game_scraper = GameScraper()
    
    # Get tournament info
    tournament_info = tournament_scraper.get_tournament_info(tournament_id)
    
    # Save to database
    tournament_db_id = db_connection.save_tournament(tournament_info)
    
    # Get player results
    results = tournament_scraper.get_tournament_results(tournament_id)
    
    # Process each player
    for result in results:
        player_info = player_scraper.get_player_details(
            tournament_id, result['start_rank']
        )
        player_db_id = db_connection.save_player(player_info)
        
        # Save result
        db_connection.save_result(tournament_db_id, player_db_id, result)
        
        # Get individual games if available
        if tournament_info.get('has_individual_games', False):
            games = game_scraper.get_player_games(
                tournament_id, result['start_rank']
            )
            for game in games:
                db_connection.save_game(tournament_db_id, game)
    
    return {
        "tournament": tournament_info,
        "players_processed": len(results),
        "status": "completed"
    }
```

This architecture provides:
1. **Separation of concerns** with different scraper classes
2. **Resilient parsing** with multiple strategies
3. **Simple error handling** with basic retries
4. **Straightforward synchronous code** that's easy to debug

## Implementation Priorities

1. **FastAPI Migration**
   - Convert Flask routes to FastAPI endpoints
   - Implement Pydantic models for data validation
   - Add automatic API documentation

2. **Database Improvements**
   - Implement new schema
   - Add proper indexes
   - Develop migration scripts

3. **Scraper Enhancements**
   - Refactor scraper architecture as outlined above
   - Implement resilient parsing with multiple strategies
   - Improve error handling

4. **Frontend Simplification**
   - Leverage Next.js RSC for data fetching
   - Improve error handling
   - Enhance responsive design
   - Simplify component structure

## Fly.io Deployment Configuration

```toml
# fly.toml
app = "chess-gp-api"
primary_region = "jnb"  # Johannesburg for lower latency to Kenya

[build]
  builder = "paketobuildpacks/builder:base"

[env]
  PORT = "8080"

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 1

[mounts]
  source = "chess_gp_data"
  destination = "/data"
```

## Conclusion

The v2 rewrite plan focuses on simplicity and reliability. By migrating to FastAPI while maintaining SQLite as the database, we can improve the API without adding unnecessary complexity. The focus is on doing a few things well rather than adding fancy features.

The frontend will leverage Next.js React Server Components for data fetching, using the built-in capabilities of the framework rather than adding external libraries.

The improved scraper architecture emphasizes reliability and resilience with a straightforward, synchronous design that's easy to debug and maintain.

The deployment to Fly.io with persistent volumes provides a simple, reliable hosting solution without overcomplicating the infrastructure.

This approach prioritizes simplicity, reliability, and maintainability - creating a system that works well and is easy to understand.
